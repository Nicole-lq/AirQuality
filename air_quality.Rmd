---
title: '**Air Quality Data Analysis: A comprehensive examination**'
author: '_Nicole Lastra Quiroz_'
output:
  html_document:
    air_print: paged
  pair_document: default
  pdf_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Packages and libraries

We proceed with the installation of packages and libraries necessary for this work, using the following command: `install.packages(c("mice", "factoextra", "gridExtra"))`,considering that  `mice` will be used for handling data imputation, `factoextra` for factor analysis, and `gridExtra` for graphical layout.

Then, the following libraries are loaded:

```{r}
library("mice")
library("factoextra")
library("gridExtra")
library("tidyverse")
library("visdat")
library("dlookr")
library("e1071")
library("flextable") 
library("inspectdf") 
library("qqplotr") 
library("ggpmisc")
library("fdth")
library("PerformanceAnalytics")
library("corrplot")
```

# Data loading

We begin the data analysis by loading the CSV file, which is stored in a dataframe named `air`, using the `read.csv()` function. With this, we can observe the basic structure of the dataframe, including the column names and a few sample data rows. 

```{r}
air <- read.csv("/Users/nicolelastraquiroz/Documents/R studio/airq_dt.csv", header = T,
                     sep    = ",", 
                     dec    = "." )
head(air)
```

## Dataframe structure analysis

```{r}
dim(air)
str(air)
```

In general, the `air` dataframe contains 100 observations (rows) and 6 numeric variables (colums). This variables (5 integers and 1 numeric) are related to air quality measurements:

* `Ozono` variable represents the **concentration of ozone ($O_3$)** in the atmosphere, measured in parts per billion (ppb), which is crucial for assessing air quality and its impact on health and the environment. 

* `RadSol` indicates the **solar radiation received**, typically measured in watts per square meter ($W/m^2$), which plays a significant role in determining atmospheric conditions. 

* `Vient` variable refers to **wind speed**, measured in kilometers per hour (km/h), reflecting how air movement affects pollutant dispersion. 

* `Temp` denotes the **ambient temperature in degrees Celsius (°C)**, which can influence chemical reactions in the atmosphere and, consequently, air quality. 

* `Mes` and `Dia` represent the **month** and **day**, respectively, providing temporal context for the observations recorded in the dataset.

# Missing data study

It is important to note that the presence of NA (missing values) can interfere with subsequent analyses, so they must be considered when performing primary studies and observations of the dataframe. Hence, it is essential to generate appropriate imputations for the dataset being worked on.

In this case, although missing values (NA) were already visible in the earlier review of the dataframe, now we use `summary()` to get a quick count and check if there are null values in the dataset (initially considering that all 6 variables are numeric, as seen in the earlier review).

```{r}
summary(air)
```

As a complement to this, and considering other methods, we add a detailed review using column-wise NA counts with `colSums(is.na())`.
 
```{r}
colSums(is.na(air))
```

Also, a graphical visualization of the missing values using `visdat::vis_miss()`.

```{r}
visdat::vis_miss(air) 
```

From the three methods, it is confirmed that there are columns with NA values, specifically: `Ozono` and `RadSol`, with 18 and 4 missing values, respectively, accounting for a total of 3.7% missing data versus 96.3% valid data.

# Imputation of missing values

The missing values could be imputed using the mean of each variable, as this is a simple and effective strategy when the percentage of missing data is low, as in this case. However, we will use the `mice` package (*Multivariable Imputation by Chained Equations*), which offers a reliable and robust method for most cases of data imputation.

```{r}
imp_dt <- mice(air, m = 1, seed = 100)
air_impt <- complete(imp_dt, 1)
```

The imputation result was stored in a new dataframe called `air_impt`.

# Verification of missing values

To rule out any potential human error in the imputation process (code errors, incorrectly saved imputed data, etc.), we will check again for NA values.

```{r}
colSums(is.na(air_impt))
```

```{r}
str(air_impt)
```

In summary, it is confirmed that there are no missing values after the imputation process, so we can proceed with the next steps.

# Full diagnosis of categorical variables

Although the dataset appears to contain mainly numeric variables, `Mes`(month) and `Dia` (day) can be treated as categorical variables due to their context. Therefore, in this case, we will first transform them into categories and then analyze them accordingly.

## Transformation of month and day variables into categories

```{r}
air_impt$Mes<-factor(air_impt$Mes)
air_impt$Dia<-factor(air_impt$Dia)
```

## Review of the transformation

```{r}
str(air_impt)
```

## Graphical visualization of data types

```{r, fig.width = 6, fig.height = 4}
visdat::vis_dat(air_impt,sort_type = TRUE) 
```

## Categorical variables analysis

```{r}
ft1 <- flextable(diagnose_category(air_impt))
ft1 <- set_caption(ft1, caption = "Details of categorical variables in the dataset")
ft1
```

# Normality study of quantitative variables

```{r}

numeric_vars <- select_if(air_impt, is.numeric)
summary(numeric_vars)

```

# Graphical method analysis

## Frequency histograms with mean and median lines

To analyze the numeric variables: `Ozono`, `RadSol`, `Vient`, `Temp`, histograms will be used. These help visualize the distribution of the data and allow us to identify whether the variables follow a symmetrical distribution or are skewed.


```{r}
plots <- list()

for (col in colnames(numeric_vars)) {
  mean_val <- mean(numeric_vars[[col]], na.rm = TRUE)
  median_val <- median(numeric_vars[[col]], na.rm = TRUE)

  p <- ggplot(numeric_vars, aes_string(x = col)) + 
        geom_histogram(binwidth = 10, fill = "pink", color = "black") +
        geom_vline(xintercept = mean_val, color = "blue", linetype = "dashed", size = 1) +
        geom_vline(xintercept = median_val, color = "magenta", linetype = "dashed", size = 1) +
        ggtitle(paste(col, "histogram"))
  plots[[col]] <- p
}

n_col <- 2
do.call("grid.arrange", c(plots, ncol = n_col))
```

Vertical lines indicating the mean (blue) and median (magenta) were added to each variable, making it easier to compare both measures and identify asymmetry. If the mean and median are far apart, it is likely that the distribution is skewed, as seen with the variables `Ozono` and `RadSol`.


### Normal probability distribution graph

We first utilized auxiliary resources to generate frequency tables:

```{r}
tb.freq <- function(x){
  f_i <- as.vector(table(x)) # absolute frequency
  F_i <- cumsum(f_i) # cumulative frequency
  h_i <- f_i / length(x) # relative frequency
  H_i <- F_i / length(x) # cumulative relative frequency
  tf <- cbind(f_i, F_i, h_i, H_i)
  row.names(tf) <- names(table(x))
  tf
}

```

Then, using the created function, we generated the frequency tables based on Sturges’ rule. Additionally, intervals were defined, data cuts and groupings were generated, and the frequency tables were visualized.

```{r}
# Frequency distribution tables
for (var_name in colnames(numeric_vars)) {
  print(paste("Frequency distribution table for:", var_name))
  tabla <- fdt(numeric_vars[[var_name]], breaks = "Sturges")
  print(tabla)
  
  # Interval calculation
  interval <- (max(numeric_vars[[var_name]]) - min(numeric_vars[[var_name]])) / (1 + 3.322 * log10(nrow(numeric_vars)))
  print(paste("Calculated interval for", var_name, ":", interval))
  
  # Data cuts and grouping
  cut_var <- cut(numeric_vars[[var_name]], breaks = seq(min(numeric_vars[[var_name]]), max(numeric_vars[[var_name]]), by = interval))
  numeric_vars[[paste(var_name, "cut", sep = ".")]] <- cut_var

  # Frequency table
  print(tb.freq(cut_var))
}
```

Next, we generated normal density and cumulative probability graphs for each of the numeric variables.

```{r, fig.width = 6, fig.height = 10}

par(mfrow = c(4, 2))

for (var_name in colnames(numeric_vars)[1:4]) {
  
# Density vs normal distribution graph
  
  plot(numeric_vars[[var_name]], 
       dnorm(numeric_vars[[var_name]], 
             mean = mean(numeric_vars[[var_name]],
                         na.rm = TRUE), 
             sd = sd(numeric_vars[[var_name]],
                     na.rm = TRUE)),
       ylab = "",xlab = var_name, 
       lwd = 2, col = "pink", main = paste("Normal density: ", var_name))
  
# Cumulative probability vs normal distribution graph
  
  plot(numeric_vars[[var_name]], 
       pnorm(numeric_vars[[var_name]], 
             mean = mean(numeric_vars[[var_name]],na.rm = TRUE), 
             sd = sd(numeric_vars[[var_name]],na.rm = TRUE)),
       xlab = var_name, 
       ylab = "", 
       lwd  = 2, 
       col  = "pink", 
       main = paste("Cumulative normal P(x): ", var_name))
  
# Mean and median as vertical lines
  
  abline(v = mean(numeric_vars[[var_name]], na.rm = TRUE), 
         col = "magenta")
  abline(v = median(numeric_vars[[var_name]], na.rm = TRUE), 
         col = "blue")
  
}

```

## Mathematical Analysis

In addition to the graphical analysis, a mathematical approach is applied, defining hypotheses, and calculating `skewness` and `kurtosis` for each variable.

### Hypothesis Formulation

We define the following hypotheses:

* $H_0:$ The variable is normally distributed.
* $H_1:$ The variable is not normally distributed.

A significance level of 0.05 is applied. If the p-value is greater than this threshold, we fail to reject the null hypothesis, implying that the variable may follow a normal distribution.

### Shapiro-Wilk Test

The Shapiro-Wilk test, which is robust for small sample sizes, is performed to assess normality.

```{r}

for (col in colnames(air_impt)[1:4]) {
  cat("\nVariable:", col)
  cat("\nPrueba de Shapiro-Wilk:\n")
  print(shapiro.test(air_impt[[col]]))
}
```


### Normality summary for numerical variables

```{r}
ft2 <- flextable(normality(numeric_vars))
ft2 <- set_caption(ft2, caption = "Normality table by variables")

ft2
```

### Skewness

`Skewness` measures the symmetry of the distribution. Values close to 0 indicate that the data distribution is relatively symmetrical.

```{r}

for (col in colnames(air)[1:4]) {
  cat("\nVariable:", col)
  cat("\nSkewness:", skewness(air_impt[[col]], na.rm = TRUE))
}
```

The variable `Ozone` shows the highest skewness (around 1.26), while the other variables display low skewness values (approximately 0.3). `Vient` has positive skewness, whereas `RadSol` and `Temp` exhibit negative skewness.

### Kurtosis

`Kurtosis` assesses the "height" and "width" of the tails of the distribution. Positive kurtosis indicates heavier tails than a normal distribution, while negative kurtosis indicates lighter tails.

```{r}

for (col in colnames(air_impt)[1:4]) {
  cat("\nVariable:", col)
  cat("\nKurtosis:", kurtosis(air_impt[[col]], na.rm = TRUE))
}
```

The variables `RadSol`, `Vient`, and `Temp` display platykurtic tendencies, with `RadSol` showing the most pronounced trend. On the other hand, `Ozone` exhibits leptokurtic behavior.

# Outlier analysis

Boxplots allow for the identification of outliers, represented as points outside the "whiskers" of the graph.

```{r, fig.width = 6, fig.height = 4}

par(mfrow = c(2, 2))

boxplot(air_impt$Ozono,  main       = "Ozone boxplot", 
                         col        = "lightblue", 
                         horizontal = TRUE
                         )

boxplot(air_impt$RadSol, main       = "Sun radiation boxplot", 
                         col        = "lightpink",
                         horizontal = TRUE
                         )

boxplot(air_impt$Vient,  main       = "Wind boxplot", 
                         col        = "lightgrey", 
                         horizontal = TRUE
                         )

boxplot(air_impt$Temp,   main       = "Temperature boxplot", 
                         col        = "lightyellow", 
                         horizontal = TRUE
                         )

```

Through these boxplots, we visually identify outliers in `Ozone` and `Wind`. To analyze these outliers in detail, we use the `flextable(diagnose_outlier())` function:

```{r}
ft3 <- flextable::flextable(diagnose_outlier(air_impt))
ft3 <- set_table_properties(ft3, width = 0.75, layout = "autofit")
ft3 <- align(ft3, align = "center", part = "all")
ft3 <- set_caption(ft3, caption = "Outlier details by variables")

ft3
```

The outlier analysis provides critical information, including the number and mean values of the outliers:

* Ozone: 1 outlier with a mean of 168.
* Wind : 2 outliers with a mean of 20.4.

This data is essential for making informed decisions regarding the treatment of these outliers, such as whether to remove them or implement other adjustments.

# Conclusions

The dataset had a low percentage of missing values (3.7%), primarily in the `Ozono` and `RadSol` variables, which were effectively imputed using the mice package. The transformation of the month and day variables into categorical types confirmed their suitability for analysis, with no unexpected distribution patterns. 
The Shapiro-Wilk test and histogram analysis showed significant deviations from normality in variables like `Ozono` and `RadSol`, with Ozone having the highest skewness (1.26), suggesting the need for transformations or non-parametric modeling. Visualizations supported these findings, highlighting the potential impact of extreme events on air quality metrics.

# Future Work

Given the skewed distribution of certain variables, applying transformations (e.g., logarithmic or square root) to these variables may improve the results of further statistical or machine learning models. Additional analyses, such as multivariate relationships between variables, could provide deeper insights into the factors driving variations in air quality metrics.
